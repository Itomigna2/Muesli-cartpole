{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w04_bU_4H9g"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "!pip install gym[classic_control]\n",
        "!pip install gym[box2d] #for lunarlander\n",
        "!apt update\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install gym-notebook-wrapper\n",
        "import gnwrapper\n",
        "!nvidia-smi\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXM9qm1Oma_c",
        "outputId": "45962bd9-d8d9-42c7-cac0-340ae7753438"
      },
      "outputs": [],
      "source": [
        "##########################\n",
        "##Representation Network\n",
        "##input : raw input\n",
        "##output : hs(hidden state) \n",
        "class Representation(nn.Module): \n",
        "    def __init__(self, input_dim, output_dim, width):\n",
        "        super().__init__()\n",
        "        self.layer1 = torch.nn.Linear(input_dim, width)\n",
        "        self.layer2 = torch.nn.Linear(width, width) \n",
        "        self.layer3 = torch.nn.Linear(width, output_dim)     \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.layer3(x)        \n",
        "        return x\n",
        "\n",
        "#######################\n",
        "##Dynamics Network        \n",
        "##input : hs, action\n",
        "##output : next_hs, reward \n",
        "class Dynamics(nn.Module): \n",
        "    def __init__(self, input_dim, output_dim, width):\n",
        "        super().__init__()\n",
        "        self.layer1 = torch.nn.Linear(input_dim + 1, width)\n",
        "        self.layer2 = torch.nn.Linear(width, width) \n",
        "        self.hs_head = torch.nn.Linear(width, output_dim)\n",
        "        self.reward_head = torch.nn.Linear(width, 1)\n",
        "       \n",
        "    def forward(self, x, action):\n",
        "        action = torch.tensor([action])\n",
        "        x = torch.cat((x,action), dim=0)\n",
        "        x = self.layer1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        hs = self.hs_head(x)\n",
        "        reward = self.reward_head(x)     \n",
        "        return hs, reward\n",
        "\n",
        "######################\n",
        "##Prediction Network\n",
        "##input : hs\n",
        "##output : P, V \n",
        "class Prediction(nn.Module): \n",
        "    def __init__(self, input_dim, output_dim, width):\n",
        "        super().__init__()\n",
        "        self.layer1 = torch.nn.Linear(input_dim, width)\n",
        "        self.layer2 = torch.nn.Linear(width, width) \n",
        "        self.policy_head = torch.nn.Linear(width, output_dim)\n",
        "        self.value_head = torch.nn.Linear(width, 1) \n",
        "   \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        P = self.policy_head(x)\n",
        "        P = torch.nn.functional.softmax(P, dim=0) \n",
        "        V = self.value_head(x)      \n",
        "        return P, V\n",
        "\n",
        "#####################\n",
        "##Target network\n",
        "class Target(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, width, target=0):\n",
        "        super().__init__()\n",
        "        self.representation_network = Representation(state_dim, state_dim//2, width) \n",
        "        self.dynamics_network = Dynamics(state_dim//2, state_dim//2, width)\n",
        "        self.prediction_network = Prediction(state_dim//2, action_dim, width) \n",
        "\n",
        "########################\n",
        "##Muesli agent\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, width, target=0):\n",
        "        super().__init__()\n",
        "        self.representation_network = Representation(state_dim, state_dim//2, width) \n",
        "        self.dynamics_network = Dynamics(state_dim//2, state_dim//2, width)\n",
        "        self.prediction_network = Prediction(state_dim//2, action_dim, width) \n",
        "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=0.0003, weight_decay=0)\n",
        "        #self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=0.999)     \n",
        "        self.to(device)\n",
        "        self.train()\n",
        "\n",
        "        self.state_traj = []\n",
        "        self.action_traj = []\n",
        "        self.P_traj = []\n",
        "        self.r_traj = []      \n",
        "\n",
        "        self.state_replay = []\n",
        "        self.action_replay = []\n",
        "        self.P_replay = []\n",
        "        self.r_replay = []   \n",
        "\n",
        "        self.action_space = action_dim\n",
        "        self.env = gym.make(game_name)     \n",
        "\n",
        "    def self_play_mu(self, max_timestep=10000):        \n",
        "        #self play with 1st inferenced policy    \n",
        "        game_score = 0\n",
        "        state = self.env.reset()\n",
        "        for i in range(max_timestep):   \n",
        "            start_state = state\n",
        "            with torch.no_grad():\n",
        "                hs = self.representation_network(torch.from_numpy(state).float().to(device))\n",
        "                P, v = self.prediction_network(hs)    \n",
        "            action = np.random.choice(np.arange(self.action_space), p=P.detach().numpy())   \n",
        "            state, r, done, _ = self.env.step(action)                    \n",
        "            self.state_traj.append(start_state)\n",
        "            self.action_traj.append(action)\n",
        "            self.P_traj.append(P)\n",
        "            self.r_traj.append(r)\n",
        "            game_score += r\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # for update inference over trajectory length\n",
        "        self.state_traj.append(np.zeros_like(state))\n",
        "        self.state_traj.append(np.zeros_like(state))\n",
        "        self.r_traj.append(torch.tensor(0))\n",
        "        self.action_traj.append(np.random.randint(self.action_space))       \n",
        "\n",
        "        # traj append to replay\n",
        "        self.state_replay.append(self.state_traj)\n",
        "        self.action_replay.append(self.action_traj)\n",
        "        self.P_replay.append(self.P_traj)\n",
        "        self.r_replay.append(self.r_traj)  \n",
        "\n",
        "        return game_score \n",
        "\n",
        "    def target_performence_test(self, target, max_timestep=10000):        \n",
        "        ## self play with 1st inferenced policy    \n",
        "        game_score = 0\n",
        "        state = self.env.reset()\n",
        "        for i in range(max_timestep):   \n",
        "            start_state = state\n",
        "            with torch.no_grad():\n",
        "                hs = target.representation_network(torch.from_numpy(state).float().to(device))\n",
        "                P, v = target.prediction_network(hs)    \n",
        "            action = np.random.choice(np.arange(self.action_space), p=P.detach().numpy())   \n",
        "            state, r, done, _ = self.env.step(action)                 \n",
        "            game_score += r\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return game_score \n",
        "\n",
        "    def update_weights_mu(self, target):\n",
        "        for _ in range(20): ## number of minibatch\n",
        "            Cumul_L_total = 0\n",
        "            for epi_sel in range(6): ## number of selected episode in a batch\n",
        "                if(epi_sel>0):## replay proportion\n",
        "                    sel = np.random.randint(0,len(self.state_replay))    \n",
        "                    self.state_traj = self.state_replay[sel]\n",
        "                    self.action_traj = self.action_replay[sel]\n",
        "                    self.P_traj = self.P_replay[sel]\n",
        "                    self.r_traj = self.r_replay[sel]\n",
        "\n",
        "                ## multi step return G (orignally retrace used)\n",
        "                G = 0\n",
        "                G_arr = []\n",
        "                for r in self.r_traj[::-1]:\n",
        "                    G = 0.99 * G + r\n",
        "                    G_arr.append(G)\n",
        "                G_arr.reverse()\n",
        "                G_arr.append(torch.tensor(0))\n",
        "                G_arr.append(torch.tensor(0))\n",
        "\n",
        "                for i in np.random.randint(len(self.state_traj)-2,size=5): ## number of selected transition in a replay\n",
        "\n",
        "                    ## update inference (2 step unroll. originally 5 step unroll recommended)                \n",
        "                    first_hs = self.representation_network(torch.from_numpy(self.state_traj[i]).float().to(device))## do not have to stack more than 1 frame\n",
        "                    first_P, first_v = self.prediction_network(first_hs)      \n",
        "                    \n",
        "                    second_hs, r = self.dynamics_network(first_hs, self.action_traj[i])    \n",
        "                    second_P, second_v = self.prediction_network(second_hs)\n",
        "\n",
        "                    third_hs, r2 = self.dynamics_network(second_hs, self.action_traj[i+1])    \n",
        "                    third_P, third_v = self.prediction_network(third_hs)\n",
        "                    \n",
        "                    ## target network inference\n",
        "                    with torch.no_grad():\n",
        "                        t_first_hs = target.representation_network(torch.from_numpy(self.state_traj[i]).float().to(device))\n",
        "                        t_first_P, t_first_v = target.prediction_network(t_first_hs)                 \n",
        "\n",
        "                    ## L_pg_cmpo first term (eq.10)      \n",
        "                    importance_weight = torch.clip(first_P.gather(0,torch.tensor(self.action_traj[i]))\n",
        "                                                /(self.P_traj[i].gather(0,torch.tensor(self.action_traj[i])).item()),\n",
        "                                                0, 1\n",
        "                    )\n",
        "                    first_term = -1 * importance_weight * (G_arr[i].item() - t_first_v.item())  \n",
        "                    \n",
        "                    ##lookahead inferences (one step look-ahead to some actions to estimate q_prior, from target network)\n",
        "                    with torch.no_grad():                                \n",
        "                        r1_arr = []\n",
        "                        v1_arr = []\n",
        "                        a1_arr = []\n",
        "                        for _ in range(self.action_space): #sample <= N(action space), now N                 \n",
        "                            action1 = np.random.choice(np.arange(self.action_space), p=t_first_P.detach().numpy())#prior pi                                             \n",
        "                            hs, r1 = target.dynamics_network(t_first_hs, action1)\n",
        "                            _, v1 = target.prediction_network(hs)\n",
        "                            r1_arr.append(r1)\n",
        "                            v1_arr.append(v1) \n",
        "                            a1_arr.append(action1)\n",
        "                    \n",
        "                    ## z_cmpo_arr (eq.12)\n",
        "                    with torch.no_grad():\n",
        "                        adv_arr = []\n",
        "                        for r1, v1 in zip(r1_arr, v1_arr):\n",
        "                            adv =  r1 + 0.99 * v1 - t_first_v # adv = q_prior - v_prior. q_prior = r1 + gamma* v1\n",
        "                            adv_arr.append(adv)\n",
        "                        \n",
        "                        exp_clip_adv_arr = [torch.exp(torch.clip(adv_arr[k], -1, 1)) for k in range(self.action_space)]\n",
        "\n",
        "                        z_cmpo_arr = []\n",
        "                        for k in range(self.action_space):\n",
        "                            z_cmpo = (1 + torch.sum(torch.tensor(exp_clip_adv_arr)) - exp_clip_adv_arr[k]) / self.action_space \n",
        "                            z_cmpo_arr.append(z_cmpo)\n",
        "\n",
        "                    ## L_pg_cmpo second term (eq.11)\n",
        "                    second_term = 0\n",
        "                    for k in range(self.action_space):\n",
        "                        second_term += exp_clip_adv_arr[k]/z_cmpo_arr[k] * torch.log(first_P.gather(0, torch.tensor(a1_arr[k])))\n",
        "                    regularizer_multiplier = 5 \n",
        "                    second_term *= -1 * regularizer_multiplier / self.action_space\n",
        "\n",
        "                    ## L_pg_cmpo               \n",
        "                    L_pg_cmpo = first_term + second_term\n",
        "\n",
        "                    ## L_v               \n",
        "                    L_v = (\n",
        "                        ((first_v - G_arr[i].item())**2)/2 \n",
        "                        + ((second_v - G_arr[i+1].item())**2)/2 \n",
        "                        + ((third_v - G_arr[i+2].item())**2)/2\n",
        "                    )\n",
        "\n",
        "                    ## L_r\n",
        "                    L_r = ((r - self.r_traj[i])**2)/2 + ((r2-self.r_traj[i+1])**2)/2                \n",
        "                    \n",
        "                    ## L_m (eq 7, eq 13)\n",
        "                    L_m = 0\n",
        "                    m_adv_arr = [] \n",
        "                    with torch.no_grad(): \n",
        "                        m_hs = target.representation_network(torch.from_numpy(self.state_traj[i+1]).float().to(device))\n",
        "                        m_P, m_v = target.prediction_network(m_hs)\n",
        "                        for j in range(self.action_space):                         \n",
        "                            hs, r = target.dynamics_network(m_hs, j)    \n",
        "                            _, v = target.prediction_network(hs)\n",
        "                            m_adv = r + 0.99 * v - m_v                        \n",
        "                            m_adv_arr.append(torch.exp(torch.clip(m_adv,-1,1)))                    \n",
        "                        pi_cmpo_all = [m_P[j]*m_adv_arr[j]/(1+sum(m_adv_arr)-m_adv_arr[j])*self.action_space for j in range(self.action_space)]\n",
        "                        pi_cmpo_all=torch.tensor(pi_cmpo_all)   \n",
        "                    kl_loss = torch.nn.KLDivLoss()\n",
        "                    L_m += kl_loss(F.log_softmax(second_P, dim=0), F.softmax(pi_cmpo_all,dim=0))#input, target      \n",
        "\n",
        "                    m_adv_arr = []               \n",
        "                    with torch.no_grad(): \n",
        "                        m_hs = target.representation_network(torch.from_numpy(self.state_traj[i+2]).float().to(device))\n",
        "                        m_P, m_v = target.prediction_network(m_hs)\n",
        "                        for j in range(self.action_space):                         \n",
        "                            hs, r = target.dynamics_network(m_hs, j)    \n",
        "                            _, v = target.prediction_network(hs)\n",
        "                            m_adv = r + 0.99 * v - m_v\n",
        "                            m_adv_arr.append(torch.exp(torch.clip(m_adv,-1,1)))             \n",
        "                        pi_cmpo_all = [m_P[j]*m_adv_arr[j]/(1+sum(m_adv_arr)-m_adv_arr[j])*self.action_space for j in range(self.action_space)]\n",
        "                        pi_cmpo_all=torch.tensor(pi_cmpo_all)    \n",
        "                    kl_loss = torch.nn.KLDivLoss()\n",
        "                    L_m += kl_loss(F.log_softmax(third_P, dim=0) , F.softmax(pi_cmpo_all,dim=0)) \n",
        "                    L_m /= 2  \n",
        "                    \n",
        "                    L_total = L_pg_cmpo + L_v/3/4 + L_r/2 + L_m/4                 \n",
        "\n",
        "                    Cumul_L_total += L_total\n",
        "\n",
        "            Cumul_L_total /= 30\n",
        "            self.optimizer.zero_grad()\n",
        "            Cumul_L_total.backward()\n",
        "\n",
        "            nn.utils.clip_grad_value_(self.parameters(), clip_value=1.0)\n",
        "\n",
        "            ## dynamics network gradient scale 1/2   \n",
        "            for d in self.dynamics_network.parameters():\n",
        "                d.grad *= 0.5\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            ## target network(prior parameters) moving average update\n",
        "            alpha_target = 0.01 \n",
        "            params1 = self.named_parameters()\n",
        "            params2 = target.named_parameters()\n",
        "            dict_params2 = dict(params2)\n",
        "            for name1, param1 in params1:\n",
        "                if name1 in dict_params2:\n",
        "                    dict_params2[name1].data.copy_(alpha_target*param1.data + (1-alpha_target)*dict_params2[name1].data)\n",
        "            target.load_state_dict(dict_params2)\n",
        "\n",
        "        #self.scheduler.step()\n",
        "\n",
        "        ##trajectory clear\n",
        "        self.state_traj.clear()\n",
        "        self.action_traj.clear()\n",
        "        self.P_traj.clear()\n",
        "        self.r_traj.clear()\n",
        "        return\n",
        "        \n",
        "device = torch.device('cpu')\n",
        "score_arr = []\n",
        "game_name = 'CartPole-v1'  \n",
        "env = gym.make(game_name) \n",
        "target = Target(env.observation_space.shape[0], env.action_space.n, 128)\n",
        "agent = Agent(env.observation_space.shape[0], env.action_space.n, 128)  \n",
        "print(agent)\n",
        "env.close()\n",
        "\n",
        "target.load_state_dict(agent.state_dict())\n",
        "\n",
        "#Self play, weight update\n",
        "episode_nums = 1000 \n",
        "for i in range(episode_nums):    \n",
        "    if i%30==0:             \n",
        "        params1 = agent.named_parameters()\n",
        "        params2 = target.named_parameters()\n",
        "        dict_params2 = dict(params2)\n",
        "        for name1, param1 in params1:\n",
        "            if name1 in dict_params2:\n",
        "                dict_params2[name1].data.copy_(0.5*param1.data + 0.5*dict_params2[name1].data)\n",
        "        target.load_state_dict(dict_params2)\n",
        "\n",
        "    game_score = agent.self_play_mu()       \n",
        "    score_arr.append(game_score)  \n",
        "\n",
        "    if i%10==0:\n",
        "        print('episode', i)    \n",
        "        print('score', game_score)\n",
        "        t_game_score = agent.target_performence_test(target)\n",
        "        print('t_score', t_game_score)\n",
        "\n",
        "    if np.mean(np.array(score_arr[i-5:i])) > 400:\n",
        "        torch.save(agent.state_dict(), 'weights.pt')\n",
        "        break\n",
        "\n",
        "    agent.update_weights_mu(target) \n",
        "\n",
        "torch.save(agent.state_dict(), 'weights.pt') \n",
        "agent.env.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "_1Cz8LkVy4du",
        "outputId": "fc219a17-438a-4691-b8e8-e232cf64eac0"
      },
      "outputs": [],
      "source": [
        "## Earned score per episode\n",
        "plt.plot(score_arr, label ='score')\n",
        "plt.legend(loc='upper left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "KTGyfRFEzUHG",
        "outputId": "5ed50002-b58a-4d6c-fa23-45ae190b5a3a"
      },
      "outputs": [],
      "source": [
        "## game play video\n",
        "agent.load_state_dict(torch.load(\"weights.pt\"))\n",
        "env = gnwrapper.LoopAnimation(gym.make(game_name)) \n",
        "state = env.reset()\n",
        "for _ in range(100):\n",
        "    with torch.no_grad():\n",
        "        hs = agent.representation_network(torch.from_numpy(state).float().to(device))\n",
        "        P, v = agent.prediction_network(hs)\n",
        "        action = np.random.choice(np.arange(agent.action_space), p=P.detach().numpy())   \n",
        "    env.render()\n",
        "    state, rew, done, _ = env.step(action.item())\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "env.display()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
